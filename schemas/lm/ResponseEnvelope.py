from pydantic import BaseModel, Field
from typing import Optional
from schemas.lm.ChatResponse import ChatResponse
from pydantic import BaseModel
from typing import Optional, List


class Usage(BaseModel):
    """
    Token and timing usage metrics for LLM calls.

    - prompt_tokens: number of tokens sent to the LLM.
    - completion_tokens: number of tokens generated by the LLM.
    - total_tokens: sum of prompt and completion tokens.
    - latency_ms: round-trip latency in milliseconds.
    """
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    latency_ms: int


class Diagnostics(BaseModel):
    """
    Broader post-processing diagnostics including usage and other optional metrics.
    """
    usage: Usage
    asr_confidence: Optional[float] = None     # ASR confidence score
    asr_latency_ms: Optional[int] = None      # ASR processing time
    tts_latency_ms: Optional[int] = None      # TTS processing time
    memory_hits: Optional[int] = None         # number of memory entries retrieved
    retrieval_latency_ms: Optional[int] = None# retrieval search time
    warnings: Optional[List[str]] = None      # post-processing warnings

class ModelInfo(BaseModel):
    """
    Information about the LLM model used for generation.
    """
    model_name: str                # e.g., "Qwen2.5-3B"
    model_version: Optional[str] = None  # e.g., "2025-05-01"


class SafetyFlags(BaseModel):
    """
    Safety and content‚Äêfiltering indicators.
    """
    filtered: Optional[bool] = None       # whether content was redacted
    flags: Optional[List[str]] = None     # e.g., ["contains_pii"]

class LengthMetrics(BaseModel):
    """
    Length and duration estimates for the response.
    """
    response_length_chars: int  # number of characters
    response_length_words: int  # approximated by split on whitespace
    estimated_audio_duration_ms: Optional[int] = None  # if TTS, duration in ms


class FallbackInfo(BaseModel):
    """
    Details about any fallback logic applied.
    """
    fallback_used: bool                   # true if a fallback path was invoked
    error_code: Optional[str] = None      # e.g., "timeout", "parse_error"
    fallback_reason: Optional[str] = None # human-readable cause


class GenerationParams(BaseModel):
    """
    Parameters controlling the LLM generation.
    """
    temperature: Optional[float] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    max_new_tokens: Optional[int] = None
    seed: Optional[int] = None


class ResponseEnvelope(BaseModel):
    """
    Complete wrapper for an LLM turn, combining:
      - model_info: metadata about which model/version run
      - generation_params: tuning parameters used
      - response: the structured ChatResponse from the LLM
      - diagnostics: token usage, latencies, ASR/TTS metrics
      - safety: content filtering flags
      - length_metrics: char/word counts, audio duration
      - fallback: any fallback logic applied
    """
    model_info: ModelInfo
    generation_params: GenerationParams
    response: ChatResponse
    diagnostics: Diagnostics

    safety: Optional[SafetyFlags]   = None
    length_metrics: Optional[LengthMetrics] = None
    fallback: Optional[FallbackInfo]       = None