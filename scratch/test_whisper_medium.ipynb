{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "f26857e50d6e4896"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T23:19:12.280377Z",
     "start_time": "2025-09-20T23:19:12.264469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, torchaudio, soundfile as sf\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ],
   "id": "2db533b2d8ccf637",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T23:23:40.069414Z",
     "start_time": "2025-09-20T23:23:40.059131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- 1. Load your WAV (British female reference) -----\n",
    "ref_wav_path = \"/Users/saketm10/Downloads/videoplayback (mp3cut.net).wav\"   # <-- your voice sample\n",
    "wav, sr = torchaudio.load(ref_wav_path)\n",
    "\n",
    "# Ensure mono\n",
    "if wav.shape[0] > 1:\n",
    "    wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "\n",
    "# Resample to 16k (required by speaker encoder)\n",
    "if sr != 16000:\n",
    "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "\n",
    "# Normalize amplitude\n",
    "wav = wav / (wav.abs().max() + 1e-9)\n",
    "\n"
   ],
   "id": "c4e89d065267ff41",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T23:24:14.854173Z",
     "start_time": "2025-09-20T23:24:12.734240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----- 2. Get 512-dim speaker embedding -----\n",
    "spk_encoder = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-xvect-voxceleb\",\n",
    "    run_opts={\"device\": device}\n",
    ")\n",
    "with torch.no_grad():\n",
    "    emb = spk_encoder.encode_batch(wav.to(device)).squeeze(0)  # shape [512]\n",
    "speaker_embedding = emb.unsqueeze(0).to(device)  # shape [1, 512]\n",
    "\n"
   ],
   "id": "238d21178675612",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T23:24:29.079597Z",
     "start_time": "2025-09-20T23:24:29.076778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----- 3. Load SpeechT5 + vocoder -----\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n"
   ],
   "id": "398ec4b94084a349",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1146026170.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31m.\u001B[39m\n    ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T23:24:43.211322Z",
     "start_time": "2025-09-20T23:24:37.150540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----- 3. Load SpeechT5 + vocoder -----\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
    "\n",
    "# ----- 4. Generate TTS -----\n",
    "text = \"Good evening, Saket. This is your assistant speaking with a British accent.\"\n",
    "inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    speech = model.generate_speech(\n",
    "        inputs[\"input_ids\"],\n",
    "        speaker_embeddings=speaker_embedding,\n",
    "        vocoder=vocoder\n",
    "    )\n",
    "\n",
    "sf.write(\"british_female_tts.wav\", speech.cpu().numpy(), 16000)\n",
    "print(\"✅ Saved british_female_tts.wav\")"
   ],
   "id": "3923f46d012af86c",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1, 1, 1, 512]}, size=[-1, 1, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m inputs = processor(text=text, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m).to(device)\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     speech = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_speech\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput_ids\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m        \u001B[49m\u001B[43mspeaker_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mspeaker_embedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvocoder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvocoder\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m sf.write(\u001B[33m\"\u001B[39m\u001B[33mbritish_female_tts.wav\u001B[39m\u001B[33m\"\u001B[39m, speech.cpu().numpy(), \u001B[32m16000\u001B[39m)\n\u001B[32m     18\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m✅ Saved british_female_tts.wav\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/env/py3.13.0/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/env/py3.13.0/lib/python3.13/site-packages/transformers/models/speecht5/modeling_speecht5.py:2785\u001B[39m, in \u001B[36mSpeechT5ForTextToSpeech.generate_speech\u001B[39m\u001B[34m(self, input_ids, speaker_embeddings, attention_mask, threshold, minlenratio, maxlenratio, vocoder, output_cross_attentions, return_output_lengths)\u001B[39m\n\u001B[32m   2780\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2781\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   2782\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mThe first dimension of speaker_embeddings must be either 1 or the same as batch size.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2783\u001B[39m             )\n\u001B[32m-> \u001B[39m\u001B[32m2785\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_generate_speech\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2786\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2787\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2788\u001B[39m \u001B[43m    \u001B[49m\u001B[43mspeaker_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2789\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2790\u001B[39m \u001B[43m    \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2791\u001B[39m \u001B[43m    \u001B[49m\u001B[43mminlenratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2792\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaxlenratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2793\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvocoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2794\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_cross_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2795\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_output_lengths\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2796\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/env/py3.13.0/lib/python3.13/site-packages/transformers/models/speecht5/modeling_speecht5.py:2348\u001B[39m, in \u001B[36m_generate_speech\u001B[39m\u001B[34m(model, input_values, speaker_embeddings, attention_mask, threshold, minlenratio, maxlenratio, vocoder, output_cross_attentions, return_output_lengths)\u001B[39m\n\u001B[32m   2345\u001B[39m idx += \u001B[32m1\u001B[39m\n\u001B[32m   2347\u001B[39m \u001B[38;5;66;03m# Run the decoder prenet on the entire output sequence.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2348\u001B[39m decoder_hidden_states = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mspeecht5\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mprenet\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_sequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspeaker_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2349\u001B[39m \u001B[38;5;66;03m# Run the decoder layers on the last element of the prenet output.\u001B[39;00m\n\u001B[32m   2350\u001B[39m decoder_out = model.speecht5.decoder.wrapped_decoder(\n\u001B[32m   2351\u001B[39m     hidden_states=decoder_hidden_states[:, -\u001B[32m1\u001B[39m:],\n\u001B[32m   2352\u001B[39m     attention_mask=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2358\u001B[39m     return_dict=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   2359\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/env/py3.13.0/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/env/py3.13.0/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/env/py3.13.0/lib/python3.13/site-packages/transformers/models/speecht5/modeling_speecht5.py:694\u001B[39m, in \u001B[36mSpeechT5SpeechDecoderPrenet.forward\u001B[39m\u001B[34m(self, input_values, speaker_embeddings)\u001B[39m\n\u001B[32m    692\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m speaker_embeddings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    693\u001B[39m     speaker_embeddings = nn.functional.normalize(speaker_embeddings)\n\u001B[32m--> \u001B[39m\u001B[32m694\u001B[39m     speaker_embeddings = \u001B[43mspeaker_embeddings\u001B[49m\u001B[43m.\u001B[49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    695\u001B[39m     inputs_embeds = torch.cat([inputs_embeds, speaker_embeddings], dim=-\u001B[32m1\u001B[39m)\n\u001B[32m    696\u001B[39m     inputs_embeds = nn.functional.relu(\u001B[38;5;28mself\u001B[39m.speaker_embeds_layer(inputs_embeds))\n",
      "\u001B[31mRuntimeError\u001B[39m: expand(torch.FloatTensor{[1, 1, 1, 512]}, size=[-1, 1, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T23:27:35.899603Z",
     "start_time": "2025-09-20T23:27:27.970684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, torchaudio, soundfile as sf\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Load reference WAV (British female)\n",
    "ref_wav_path = \"/Users/saketm10/Downloads/videoplayback (mp3cut.net).wav\"   # <-- your voice sample\n",
    "wav, sr = torchaudio.load(ref_wav_path)\n",
    "if wav.shape[0] > 1:\n",
    "    wav = wav.mean(dim=0, keepdim=True)\n",
    "if sr != 16000:\n",
    "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "wav = wav / (wav.abs().max() + 1e-9)\n",
    "\n",
    "# 2) Speaker embedding -> [1, 512]\n",
    "spk_encoder = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-xvect-voxceleb\",\n",
    "    run_opts={\"device\": device}\n",
    ")\n",
    "with torch.no_grad():\n",
    "    enc = spk_encoder.encode_batch(wav.to(device))  # e.g., [1,1,1,512] or [1,512]\n",
    "enc = enc.squeeze()\n",
    "if enc.dim() == 1:\n",
    "    speaker_embedding = enc.unsqueeze(0)\n",
    "elif enc.dim() == 2 and enc.size(0) == 1:\n",
    "    speaker_embedding = enc\n",
    "else:\n",
    "    if enc.numel() == 512:\n",
    "        speaker_embedding = enc.view(1, 512)\n",
    "    else:\n",
    "        while enc.dim() > 1 and enc.size(0) > 1:\n",
    "            enc = enc.mean(dim=0, keepdim=False)\n",
    "        speaker_embedding = enc.view(1, 512)\n",
    "speaker_embedding = speaker_embedding.to(device).float()\n",
    "\n",
    "# 3) SpeechT5 + HiFiGAN\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
    "\n",
    "# 4) Synthesize\n",
    "text = \"Good evening, this is a British female voice generated using Speech T five.\"\n",
    "inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    speech = model.generate_speech(\n",
    "        inputs[\"input_ids\"],\n",
    "        speaker_embeddings=speaker_embedding,\n",
    "        vocoder=vocoder\n",
    "    )\n",
    "\n",
    "sf.write(\"british_female_tts.wav\", speech.cpu().numpy(), 16000)\n",
    "print(\"Saved: british_female_tts.wav\")"
   ],
   "id": "38bcfb5cdb0d4d8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: british_female_tts.wav\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7db1752d3e52ddd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
