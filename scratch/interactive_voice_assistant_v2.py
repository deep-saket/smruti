# interactive_voice_assistant.py

from modules.audio import AudioRecorder, AudioPlayer
from models.OpenAIWhisperTinyInfer import OpenAIWhisperTinyInfer
from models import Qwen25Infer, QwenV25Infer
from models.VITSTTSInfer import VITSTTSInfer
from prompts import PromptBuilderMain
from memory import ImmediateMemory
from memory import ShortTermMemory

def main():
    # Initialize I/O & models
    recorder = AudioRecorder(samplerate=16000, channels=1, dtype='int16')
    player   = AudioPlayer(samplerate=16000)
    stt      = OpenAIWhisperTinyInfer()
    llm      = QwenV25Infer(model_name="Qwen/Qwen2.5-VL-3B-Instruct")
    tts      = VITSTTSInfer(model_name="tts_models/en/ljspeech/vits", device="cpu")

    # Prompt builder (which also provides parser)
    builder  = PromptBuilderMain()

    # Memories
    chat_mem = ImmediateMemory(capacity=6)
    sem_mem  = ShortTermMemory(capacity=100)

    print("ðŸ¤– Interactive voice assistant (say 'exit' to quit)")

    while True:
        # 1) Capture & transcribe
        audio_in  = recorder.record(duration=5)
        user_text = stt.infer(audio_in).strip()
        print(f"\nYou said: {user_text}")

        # 2) Update memories
        chat_mem.add("user", user_text)
        sem_mem.add(user_text)

        if user_text.lower() in ("exit", "quit"):
            print("ðŸ‘‹ Goodbye!")
            break

        # 3) Ad-hoc retrieval
        if user_text.lower().startswith("what did i say about"):
            query = user_text[len("what did i say about"):].strip()
            hits  = sem_mem.get(query_text=query, k=5)
            print("ðŸ“š You said:")
            for h in hits:
                print("   ", h)
            continue

        # 4) Build prompts
        history         = "\n".join(f"{role}: {txt}" for role, txt in chat_mem.get())
        memory_snippets = "\n".join(sem_mem.get()[:5])
        memory_writes   = ""  # or populate from your logic
        context = {
            "new_message":     user_text,
            "context":         history,
            "memory_snippets": memory_snippets,
            "memory_writes":   memory_writes
        }
        prompts = builder(**context)

        # 5) Generate LLM response
        print("ðŸ’¬ Generating response...")
        system_prompt = prompts["system"]
        user_prompt   = prompts["user"]

        raw = llm.infer_lang(
            system_prompt=system_prompt,
            prompt=user_prompt,
            max_new_tokens=8000
        )
        print(f"Assistant (raw): {raw}")

        # 6) Parse via builderâ€™s cached parser
        chat_resp      = builder.parser(raw)
        assistant_text = chat_resp.messages[-1].content

        # 7) Update memories with assistantâ€™s reply
        chat_mem.add("assistant", assistant_text)
        sem_mem.add(assistant_text)

        # 8) Play back
        print("ðŸ”Š Speaking response...")
        for line in assistant_text.splitlines():
            if line.strip():
                wav = tts.infer(line)
                player.play(wav)

    print("ðŸ›‘ Session ended.")

if __name__ == "__main__":
    main()