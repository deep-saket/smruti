{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:41:52.916702Z",
     "start_time": "2025-07-17T03:41:40.695222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_path = \"nanonets/Nanonets-OCR-s\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    #device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):\n",
    "    prompt = \"\"\"Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": f\"file://{image_path}\"},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return output_text[0]\n",
    "\n",
    "image_path = \"/path/to/your/document.jpg\"\n",
    "result = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)\n",
    "print(result)\n"
   ],
   "id": "f803f3424a2693e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a9a8edea90747a7a5a37c432b48efe3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbc0f00cb0934c5fb9d8af89b2f04aae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66ec5aee22724d98a02528b60398e1d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5683045f2564df9bea9bb83110680f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3427f8771e084cfa97e56eac7ac8ce9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07925f859c3b4da3a475b5f2785f7f41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5df2f3847deb42aebbbb82e0abfacbf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3301598395e044a78060692bdea81ba7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b927ce38f8e48f9a0f59b7f5fffa428"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebd30651869d44e1869f9dfeb0a7b3e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "video_preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "643b1885aad342cf8557105f4205fddb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/document.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 38\u001B[39m\n\u001B[32m     35\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output_text[\u001B[32m0\u001B[39m]\n\u001B[32m     37\u001B[39m image_path = \u001B[33m\"\u001B[39m\u001B[33m/path/to/your/document.jpg\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m result = \u001B[43mocr_page_with_nanonets_s\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m15000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mocr_page_with_nanonets_s\u001B[39m\u001B[34m(image_path, model, processor, max_new_tokens)\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mocr_page_with_nanonets_s\u001B[39m(image_path, model, processor, max_new_tokens=\u001B[32m4096\u001B[39m):\n\u001B[32m     18\u001B[39m     prompt = \u001B[33m\"\"\"\u001B[39m\u001B[33mExtract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes.\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     image = \u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m     messages = [\n\u001B[32m     21\u001B[39m         {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33msystem\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mYou are a helpful assistant.\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m     22\u001B[39m         {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   (...)\u001B[39m\u001B[32m     25\u001B[39m         ]},\n\u001B[32m     26\u001B[39m     ]\n\u001B[32m     27\u001B[39m     text = processor.apply_chat_template(messages, tokenize=\u001B[38;5;28;01mFalse\u001B[39;00m, add_generation_prompt=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/smruti/.venv/lib/python3.11/site-packages/PIL/Image.py:3505\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(fp, mode, formats)\u001B[39m\n\u001B[32m   3502\u001B[39m     filename = os.fspath(fp)\n\u001B[32m   3504\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[32m-> \u001B[39m\u001B[32m3505\u001B[39m     fp = \u001B[43mbuiltins\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   3506\u001B[39m     exclusive_fp = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3507\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '/path/to/your/document.jpg'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T05:32:04.123888Z",
     "start_time": "2025-06-28T05:32:04.111550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\"\"\"\n",
    "One-shot speaker verification using SpeechBrain ECAPA-TDNN,\n",
    "with live audio capture via modules/audio.py.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import normalize\n",
    "from speechbrain.pretrained import SpeakerRecognition\n",
    "from modules.audio import AudioRecorder\n"
   ],
   "id": "1522c4cafdd65a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Initialize audio recorder (16 kHz mono)\n",
    "recorder = AudioRecorder(samplerate=16000, channels=1, dtype='int16')"
   ],
   "id": "fb36d1e6996f2ae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Load pretrained ECAPA-TDNN speaker-recognition model\n",
    "model = SpeakerRecognition.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"pretrained_models/spkrec-ecapa-voxceleb\"\n",
    ")"
   ],
   "id": "ec182603aa8ee82d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed_audio(wav: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Take a 1D float32 numpy array of audio at 16 kHz,\n",
    "    convert to a Torch tensor [1, time],\n",
    "    and return a normalized embedding [1, dim].\n",
    "    \"\"\"\n",
    "    tensor = torch.from_numpy(wav).unsqueeze(0)  # [1, time]\n",
    "    emb = model.encode_batch(tensor)             # [1, embed_dim]\n",
    "    return normalize(emb, p=2, dim=-1)"
   ],
   "id": "a4574a333a53a017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------------------------------------------------------------\n",
    "# ENROLLMENT PHASE: record 1–2 reference utterances to build your prototype\n",
    "# --------------------------------------------------------------------\n",
    "print(\"🎤 Enrollment: Please speak your passphrase twice, each ~3 seconds long.\")\n",
    "reference_embeddings = []\n",
    "for i in range(2):\n",
    "    wav = recorder.record(duration=3.0)         # record 3 seconds\n",
    "    emb = embed_audio(wav)\n",
    "    reference_embeddings.append(emb)"
   ],
   "id": "2703ab539095471d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# average and re-normalize to get your speaker prototype\n",
    "prototype = torch.mean(torch.stack(reference_embeddings), dim=0)\n",
    "prototype = normalize(prototype, p=2, dim=-1)\n",
    "prototype.shape"
   ],
   "id": "585c5f987b6eef6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------------------------------------------------------------\n",
    "# VERIFICATION PHASE: record a test utterance and compare against prototype\n",
    "# --------------------------------------------------------------------\n",
    "print(\"\\n🎤 Verification: Please speak your passphrase (~3 seconds).\")\n",
    "test_wav = recorder.record(duration=3.0)\n",
    "test_emb = embed_audio(test_wav)\n",
    "test_emb"
   ],
   "id": "402f37b528e37a20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_emb.shape",
   "id": "15fd6e025ae133ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compute cosine similarity score\n",
    "score = torch.matmul(test_emb[0], prototype[0].T).item()\n",
    "threshold = 0.70  # adjust after ROC analysis on your data\n",
    "\n",
    "print(f\"\\n🔍 Cosine score = {score:.3f}\")\n",
    "if score >= threshold:\n",
    "    print(\"✅ Speaker verified!\")\n",
    "else:\n",
    "    print(\"❌ Speaker not recognized.\")\n",
    "\n",
    "# (Optional) You can wrap the above into a loop or API server as needed."
   ],
   "id": "68adfd555e00b764"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T03:57:38.898090Z",
     "start_time": "2025-06-30T03:57:33.133194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "from config.loader import settings\n",
    "from models.ModelManager import ModelManager\n",
    "from modules.audio.AudioRecogniserManager import AudioRecogniserManager\n",
    "from modules.audio.io import AudioRecorder\n"
   ],
   "id": "6d6f598dda79d54c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saketm10/Projects/smruti/.venv/lib/python3.11/site-packages/jieba/_compat.py:18: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/Users/saketm10/Projects/smruti/.venv/lib/python3.11/site-packages/pkg_resources/__init__.py:3147: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "09:27:38 datasets INFO │ PyTorch version 2.7.1 available.\n",
      "09:27:38 qwen_vl_utils.vision_process INFO │ set VIDEO_TOTAL_PIXELS: 90316800\n",
      "09:27:38 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint save hook for _speechbrain_save\n",
      "09:27:38 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint load hook for _speechbrain_load\n",
      "09:27:38 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint save hook for save\n",
      "09:27:38 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint load hook for load\n",
      "09:27:38 speechbrain.utils.quirks INFO │ Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "09:27:38 speechbrain.utils.quirks INFO │ Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "09:27:38 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint save hook for _save\n",
      "09:27:38 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint load hook for _recover\n",
      "/Users/saketm10/Projects/smruti/models/SpeakerEmbeddingInfer.py:4: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import SpeakerRecognition\n",
      "09:27:38 faiss.loader INFO │ Loading faiss.\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
      "09:27:38 faiss.loader INFO │ Successfully loaded faiss.\n",
      "09:27:38 faiss INFO │ Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T03:58:03.341611Z",
     "start_time": "2025-06-30T03:57:38.921605Z"
    }
   },
   "cell_type": "code",
   "source": "ModelManager()",
   "id": "251e792d3bef172b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:27:38 ModelManager INFO │ Project Root: /Users/saketm10/Projects/smruti\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-VL-3B-Instruct model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b29beaab26b44cb95141cba10290a3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      " > tts_models/en/ljspeech/vits is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      "Loading GLiNER model 'urchade/gliner_base' on mps...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "573eaddd28d1477da0e64f58e4fa950d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saketm10/Projects/smruti/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "09:27:57 sentence_transformers.SentenceTransformer INFO │ Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:28:01 speechbrain.utils.fetching INFO │ Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "09:28:01 speechbrain.utils.fetching INFO │ Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "09:28:01 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint save hook for _save\n",
      "09:28:01 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint load hook for _load\n",
      "09:28:01 speechbrain.utils.checkpoints DEBUG │ Registered parameter transfer hook for _load\n",
      "/Users/saketm10/Projects/smruti/.venv/lib/python3.11/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "09:28:01 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint save hook for save\n",
      "09:28:01 speechbrain.utils.checkpoints DEBUG │ Registered checkpoint load hook for load_if_possible\n",
      "09:28:01 speechbrain.utils.parameter_transfer DEBUG │ Fetching files for pretraining (no collection directory set)\n",
      "09:28:01 speechbrain.utils.fetching INFO │ Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "09:28:02 speechbrain.utils.parameter_transfer DEBUG │ Set local path in self.paths[\"embedding_model\"] = /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
      "09:28:02 speechbrain.utils.fetching INFO │ Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "09:28:02 speechbrain.utils.parameter_transfer DEBUG │ Set local path in self.paths[\"mean_var_norm_emb\"] = /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
      "09:28:02 speechbrain.utils.fetching INFO │ Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "09:28:02 speechbrain.utils.parameter_transfer DEBUG │ Set local path in self.paths[\"classifier\"] = /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
      "09:28:02 speechbrain.utils.fetching INFO │ Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "09:28:03 speechbrain.utils.parameter_transfer DEBUG │ Set local path in self.paths[\"label_encoder\"] = /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
      "09:28:03 speechbrain.utils.parameter_transfer INFO │ Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "09:28:03 speechbrain.utils.parameter_transfer DEBUG │ Redirecting (loading from local path): embedding_model -> /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
      "09:28:03 speechbrain.utils.parameter_transfer DEBUG │ Redirecting (loading from local path): mean_var_norm_emb -> /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
      "09:28:03 speechbrain.utils.parameter_transfer DEBUG │ Redirecting (loading from local path): classifier -> /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
      "09:28:03 speechbrain.utils.parameter_transfer DEBUG │ Redirecting (loading from local path): label_encoder -> /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
      "09:28:03 speechbrain.dataio.encoder DEBUG │ Loaded categorical encoding from /Users/saketm10/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.ModelManager.ModelManager at 0x37129a750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T03:58:13.580802Z",
     "start_time": "2025-06-30T03:58:13.540853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_recogniser = AudioRecogniserManager(\n",
    "    ModelManager.speaker_embedder,\n",
    "    embedding_dim=ModelManager.speaker_embedder.embedding_dim,\n",
    "    db_path=settings['db']['audio_recogniser']\n",
    ")\n"
   ],
   "id": "f05130a0ce97a0c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".cache/audio_embeddings.npz ##########\n",
      "float32 (1, 384)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m audio_recogniser = \u001B[43mAudioRecogniserManager\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mModelManager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mspeaker_embedder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_dim\u001B[49m\u001B[43m=\u001B[49m\u001B[43mModelManager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mspeaker_embedder\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdb_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43msettings\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdb\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43maudio_recogniser\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/smruti/modules/audio/AudioRecogniserManager.py:28\u001B[39m, in \u001B[36mAudioRecogniserManager.__init__\u001B[39m\u001B[34m(self, embedding_component, embedding_dim, db_path)\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m os.path.exists(\u001B[38;5;28mself\u001B[39m.db_path):\n\u001B[32m     27\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m.db_path, \u001B[33m\"\u001B[39m\u001B[33m##########\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_load_database\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/smruti/modules/audio/AudioRecogniserManager.py:39\u001B[39m, in \u001B[36mAudioRecogniserManager._load_database\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     37\u001B[39m     \u001B[38;5;28mself\u001B[39m.db[sid] = vec\n\u001B[32m     38\u001B[39m     \u001B[38;5;28mself\u001B[39m.metadata[sid] = name\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfaiss\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43membs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/smruti/modules/db/FAISSIndexManager.py:23\u001B[39m, in \u001B[36mFAISSIndexManager.add\u001B[39m\u001B[34m(self, vecs, labels)\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[33;03mvecs: (N, dim) float32, normalized\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[33;03mlabels: length-N list of unique IDs\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;28mprint\u001B[39m(vecs.dtype, vecs.shape)\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m vecs.dtype == np.float32 \u001B[38;5;129;01mand\u001B[39;00m vecs.shape[-\u001B[32m1\u001B[39m] == \u001B[38;5;28mself\u001B[39m.dim\n\u001B[32m     24\u001B[39m \u001B[38;5;28mself\u001B[39m.index.add(vecs)\n\u001B[32m     25\u001B[39m \u001B[38;5;28mself\u001B[39m.ids.extend(labels)\n",
      "\u001B[31mAssertionError\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:24:02.834718Z",
     "start_time": "2025-06-29T07:23:47.665533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_utts = int(input(\"How many reference utterances to record? \"))\n",
    "\n",
    "# Collect WAVs\n",
    "recorder = AudioRecorder(samplerate=16000, channels=1, dtype='int16')\n",
    "wavs = []\n",
    "for i in range(n_utts):\n",
    "    input(f\"Press Enter to record utterance #{i+1}...\")\n",
    "    wav = recorder.record(duration=3.0)\n",
    "    wavs.append(wav.astype(np.float32) / np.iinfo(np.int16).max)\n"
   ],
   "id": "eebca235ef844f2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎙️  Recording for 3.0 seconds...\n",
      "🎙️  Recording for 3.0 seconds...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:24:16.161849Z",
     "start_time": "2025-06-29T07:24:05.107995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "name = input(\"Enter speaker name: \").strip()\n",
    "provided_id = input(\"Enter speaker ID (or leave empty to auto-generate): \").strip() or None\n"
   ],
   "id": "8cc9b61fea0d867b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:26:38.348923Z",
     "start_time": "2025-06-29T07:26:38.279241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Enroll and persist\n",
    "sid = audio_recogniser.enroll(name=name, wavs=wavs, speaker_id=provided_id)\n",
    "print(f\"✅ Enrolled '{name}' with speaker_id = {sid}\")\n"
   ],
   "id": "e5a1378fded08c72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enrolled 'Saket Mohanty' with speaker_id = de61be0b612a400c8f5ef4de17b749a1\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:24:22.134360Z",
     "start_time": "2025-06-29T07:24:22.131586Z"
    }
   },
   "cell_type": "code",
   "source": "audio_recogniser.faiss.dim",
   "id": "81e2a46e7d3d024c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:24:23.131427Z",
     "start_time": "2025-06-29T07:24:23.064212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embs = [audio_recogniser.embedder.embed(w) for w in wavs]\n",
    "proto = np.mean(np.stack(embs, axis=0), axis=0)\n",
    "proto = proto.astype(np.float32) / np.linalg.norm(proto)"
   ],
   "id": "90b05fc714e9cc7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:24:24.431007Z",
     "start_time": "2025-06-29T07:24:24.428171Z"
    }
   },
   "cell_type": "code",
   "source": "proto.shape",
   "id": "eb7129bff64ccf18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 192)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:26:34.054697Z",
     "start_time": "2025-06-29T07:26:34.052714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.makedirs(os.path.dirname(settings['db']['audio_recogniser']), exist_ok=True)"
   ],
   "id": "8147b182a80e537e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2f5ff9fc26e342e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
